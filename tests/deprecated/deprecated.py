import numpy as np
from tests.tesisfunctions import separatingLine,anorm,contour2points,vectorsAngles,normsigmoid

def angle(v1,v2, deg = False):
    """
    Angle between twe vectors.

    :param v1: vector 1.
    :param v2: vector 2.
    :param deg: if True angle is in Degrees, else radians.
    :return: angle in radians.

    .. warning:: this algorithm is incorrect as explained in http://stackoverflow.com/a/13849249/5288758
    """
    # v1.v2 = ||v1||||v2|| cos(angle) => angle = arcos(v1.v2/||v1||||v2||)
    # see more: http://www.wikihow.com/Find-the-Angle-Between-Two-Vectors
    # tested with http://codereview.stackexchange.com/a/54413
    if deg: return np.rad2deg(np.arccos(np.dot(v1,v2)/(anorm(v1)*anorm(v2)))) # *180.0/np.pi
    return np.arccos(np.dot(v1,v2)/(anorm(v1)*anorm(v2)))

def normalize(data):
    """normalize data to [0,1]"""
    min = np.min(data)
    if min:
        data = data + min
        return data/np.max(data)
    else: # if min is 0
        return data/np.max(data)

def normalize2(data):
    """normalize data to [-1,1]"""
    return data/np.max([np.max(data),-1.0*np.min(data)])

def normalizeToRange(data,max=255,min=0):
    """
    Normalize to custom range.

    :param data: value or array
    :param max: max value of range
    :param min: min value of range
    :return: normalized data.
    """
    if min: return (max-min)*normalize(data)+min
    else: return max*normalize2(data)  # speeds up operation

def separeByDefect_BAD(cnt,defects): # BAD, explanation in comments
    """
    Separate contour at two defects with biggest distances. (contours can be un unordered)

    :param cnt: contour
    :param defects: convexity defects
    :return: contour a side, contour at the other side
    """
    # this is badly made, if by any chance the separating line pass trough more than 2
    # contours then some of those contours will be miss judged and the idea
    # is to separate two objects as a hole not to generate more objects or place
    # corners were are not connected to the main object.
    p1,p2 = separatingLine(cnt,defects) # get points to form vector
    pts = contour2points(cnt) # convert contour to points
    angles = np.nan_to_num(vectorsAngles(pts,p2,p1,absolute=False)) # get angles
    return cnt[angles >= 0], cnt[angles < 0] # split at threshold

def filter(alfa,beta1,beta2=None):
    """
    Make filter.

    :param alfa: steepness of filter
    :param beta1: first shift from origin
    :param beta2: second shift from origin::

        alfa must be != 0
        if beta2 = None:
            if alfa > 0: high-pass filter, if alfa < 0: low-pass filter
        else:
            if beta2 > beta1:
                if alfa > 0: band-pass filter, if alfa < 0: band-stop filter
            else:
                if alfa > 0: inverted-band-pass filter, if alfa < 0: inverted-band-stop filter
    :return: filter funtion with intup levels

    Example::

        alfa,beta1,beta2 = 10,20,100
        myfilter = filter(alfa,beta1,beta2)
        print myfilter,type(myfilter)
        print myfilter.alfa,myfilter.beta1,myfilter.beta2
    """
    #http://en.wikipedia.org/wiki/Filter_%28signal_processing%29
    def bandstop(levels):
        return normsigmoid(levels,alfa,beta1)-normsigmoid(levels,alfa,beta2)+1.0
    def bandpass(levels):
        return normsigmoid(levels,alfa,beta1)-normsigmoid(levels,alfa,beta2)
    def invertedbandstop(levels):
        return normsigmoid(levels,alfa,beta2)-normsigmoid(levels,alfa,beta1)-1.0
    def invertedbandpass(levels):
        return normsigmoid(levels,alfa,beta2)-normsigmoid(levels,alfa,beta1)
    def lowpass(levels):
        return normsigmoid(levels,alfa,beta1)
    def highpass(levels):
        return normsigmoid(levels,alfa,beta1)
    if beta2 is None:
        if alfa < 0: # low pass
            func = lowpass
        else: # high pass
            func = highpass
    else:
        if beta2>beta1:
            if alfa < 0: # band stop
                func = bandstop
            else: # band pass
                func = bandpass
        else: # inverted
            temp = beta1
            beta1 = beta2
            beta2 = temp
            if alfa < 0: # inverted band stop
                func = invertedbandstop
            else: # inverted band pass
                func = invertedbandpass
    func.func_dict = {"alfa":alfa,"beta1":beta1,"beta2":beta2}
    return func

def getparameters(filter,title = ""):
    """
    used in filter generated by function filter.

    :param filter:
    :param title:
    :return:
    """
    vardic = filter.func_dict
    for i in vardic.keys():
        if vardic[i] is not None:
            title += " "+i+": "
            title += str(vardic[i])+","
    return title[:-1]

def instability_bf(funcs, step = 10, maximum = 300, guess = 0, tolerance=0.01):
    """
    Find the instability of function approaching value by brute force,

    :param funcs: list of functions
    :param step: (10) step to close guess to maximum
    :param maximum: (300) maximum value, if guess surpass this value then calculations are stopped.
    :param guess: (0) initial guess
    :param tolerance: (0.01) tolerance with last step to check instability.
    :return: (state, updated guess). state is True if successful, else False.
    """
    if guess < maximum:
        s = 1 # to increase
    else:
        s = -1 # to decrease
    step = s*abs(step) # correct step
    # offset to ensure that data moves to maximum even if actual data is stable
    offset = [f(maximum) for f in funcs]
    val_prev = np.array([f(guess-step) for f in funcs]+offset) # first y values with offset
    acc = 0 # accumulator to interchange when to add offset and when not
    while s*(maximum-guess)>0: # check approximation to maximum
        val = [f(guess) for f in funcs] # get y values
        if acc%2: # interchange
            val = np.array(val+offset) # values with offset
        else:
            val = np.array(val+val) # just values
        # np.repeat(np.mean(val),val.size)
        # check minimization
        if np.allclose(val, val_prev, tolerance, tolerance): # it means instability
            return True, guess # success!
        guess += step # updata step
        acc += 1 # update accumulator
        val_prev = val # update previous data
    return False, guess # not found or limit reached

def matchExplorer(win, img1, img2, kp_pairs=(), status = None, H = None, show=True, block= True, daemon=True):
    """
    This function draws a set of keypoint pairs obtained on a match method of a descriptor
    on two images imgf and imgb. (backend: plotim).

    :param win: window's name (str)
    :param img1: image1 (numpy array)
    :param img2: image2 (numpy array)
    :param kp_pairs: zip(keypoint1, keypoint2)
    :param status: obtained from cv2.findHomography
    :param H: obtained from cv2.findHomography (default=None)
    :param show: if True shows plotim using block and daemon, else do not show
    :param block: if True it wait for window close, else it detaches
    :param daemon: if True window closes if main thread ends, else windows must be closed to main thread to end
    :return: plotim object with visualization as self.rimg (image with matching result) (default=None)

    .. note:: It supports BGR and gray images.
    """
    # FIXME keypoints visualization wrong
    # functions
    ## GET INITIAL VISUALIZATION
    if len(img1.shape)<3:
        img1 = cv2.cvtColor(img1,cv2.COLOR_GRAY2BGR)
    if len(img2.shape)<3:
        img2 = cv2.cvtColor(img2,cv2.COLOR_GRAY2BGR)

    h1, w1 = img1.shape[:2]  # obtaining image1 dimensions
    h2, w2 = img2.shape[:2]  # obtaining image2 dimensions
    # imgf and imgb will be visualized horizontally (left-right)
    vis = np.zeros((max(h1, h2), w1+w2,3), np.uint8)  # making visualization image
    vis[:h1, :w1] = img1  # imgf at the left of vis
    vis[:h2, w1:w1+w2] = img2  # imgf at the right of vis

    if status is None:
        status = np.ones(len(kp_pairs), np.bool_)  # making sure every pair of keypoints is graphed

    kp_pairs = [(dict2keyPoint(i),dict2keyPoint(j)) for i,j in kp_pairs]
    p1 = FLOAT([kpp[0].pt for kpp in kp_pairs])  # pair of coordinates for imgf
    p2 = FLOAT([kpp[1].pt for kpp in kp_pairs]) + (w1, 0) # pair of coordinates for imgb

    if H is not None:# does the same as getTransformedCorners
        corners = FLOAT([[0, 0], [w1, 0], [w1, h1], [0, h1]])
        corners = np.int32(cv2.perspectiveTransform(corners.reshape(1, -1, 2), H).reshape(-1, 2) + (w1, 0))

    def drawline(self):
        vis = self.rimg
        self.thick = int(sigmoid(vis.shape[0] * vis.shape[1], 1723567, 8080000, 5, 1))
        if H is not None:  # enclosing object
            rcorners = np.array([self.real2render(corner[0],corner[1]) for corner in corners],np.int32)
            cv2.polylines(vis, [rcorners], True, self.framecolor) # draw rendered TM encasing

        rp1,rp2 = [],[]
        for (x1, y1), (x2, y2), inlier in zip(p1, p2, status):
            rx1,ry1 = self.real2render(x1,y1,np.int32) # real to render
            rx2,ry2 = self.real2render(x2,y2,np.int32) # real to render
            rp1.append((rx1,ry1))
            rp2.append((rx2,ry2))
            r = self.thick
            if inlier and self.showgoods:  # drawing circles (good keypoints)
                col = self.goodcolor
                cv2.circle(vis, (rx1, ry1), r, col, -1)  # for left keypoint (imgf)
                cv2.circle(vis, (rx2, ry2), r, col, -1)  # for right keypoint (imgf)
            elif self.showbads:  # drawing x marks (wrong keypoints)
                col = self.badcolor
                thickness = r+5
                # for left keypoint (imgf)
                cv2.line(vis, (rx1-r, ry1-r), (rx1+r, ry1+r), col, thickness)
                cv2.line(vis, (rx1-r, ry1+r), (rx1+r, ry1-r), col, thickness)
                # for right keypoint (imgf)
                cv2.line(vis, (rx2-r, ry2-r), (rx2+r, ry2+r), col, thickness)
                cv2.line(vis, (rx2-r, ry2+r), (rx2+r, ry2-r), col, thickness)
            # drawing lines for non-onmouse event
        self.rp1 = np.int32(rp1)
        self.rp2 = np.int32(rp2)
        self.vis0 = vis.copy()  # saving state of the visualization for onmouse event
        # get rendered kp_pairs
        self.kp_pairs2 = apply2kp_pairs(kp_pairs,self.real2render,self.real2render)
        # drawing lines for non-onmouse event
        for (rx1, ry1), (rx2, ry2), inlier in zip(rp1, rp2, status):
            if inlier and self.showgoods:
                cv2.line(vis, (rx1, ry1), (rx2, ry2), self.goodcolor,r)
        self.vis = vis#.copy() # visualization with all inliers

    def drawrelation(self):
        if self.flags & cv2.EVENT_FLAG_LBUTTON:
            x,y = self.rx, self.ry
            cur_vis = self.vis0.copy()  # actual visualization
            r = self.thick + 8  # proximity to keypoint
            m = (anorm(self.rp1 - (x, y)) < r) | (anorm(self.rp2 - (x, y)) < r)
            idxs = np.where(m)[0]  # get indexes near pointer
            kp1s, kp2s = [], []
            for i in idxs:  # for all keypints near pointer
                (rx1, ry1), (rx2, ry2) = self.rp1[i], self.rp2[i]  # my keypoint
                col = (self.badcolor, self.goodcolor)[status[i]]  # choosing False=red,True=green
                cv2.line(cur_vis, (rx1,ry1), (rx2,ry2), col, self.thick)  # drawing line
                # keypoints to show on event
                kp1, kp2 = self.kp_pairs2[i]
                kp1s.append(kp1)
                kp2s.append(kp2)
            # drawing keypoints near pointer for imgf and imgb
            cur_vis = cv2.drawKeypoints(cur_vis, kp1s, flags=4, color=self.kpcolor)
            cur_vis = cv2.drawKeypoints(cur_vis, kp2s, flags=4, color=self.kpcolor)
            self.rimg = cur_vis
        else:
            self.rimg = self.vis

        if self.y is not None and self.x is not None:
            self.builtinplot(self.sample[self.y,self.x])

    def randomColor():
        return (np.random.randint(0,255), np.random.randint(0,255), np.random.randint(0,255))

    def mousefunc(self):
        if self.builtincontrol():
            self.updaterenderer()
            drawline(self)

        drawrelation(self)

    def keyfunc(self):
        if self.builtincmd():
            drawline(self)
            drawrelation(self)
            if self.y is not None and self.x is not None:
                self.builtinplot(self.img[self.y,self.x])
            else:
                self.builtinplot()

    self = plotim(win, vis)
    self.mousefunc = mousefunc
    self.keyfunc = keyfunc
    self.showgoods = True
    self.showbads = False
    from image import colors
    self.__dict__.update(colors)
    self.randomColor = randomColor
    self.goodcolor = self.green
    self.badcolor = self.red
    self.kpcolor = self.orange
    self.framecolor = self.blue
    self.cmdlist.extend(["showgoods","showbads","framecolor","kpcolor","badcolor","goodcolor"])
    drawline(self)
    # show window
    if show: self.show(block= block, daemon=daemon)
    return self #self.rimg # return coordinates

def getcoor(im, win = "get coordinates", updatefunc=drawcoorpoints, prox=8, radius = 3, unique=True, col_out=black,col_in=red):
    """
    Create window to select points from image.

    :param im: image to get points.
    :param win: window name.
    :param updatefunc: function to draw interaction with points.(e.g. limitaxispoints, drawcoorperspective, etc.).
    :param prox: proximity to identify point.
    :param radius: radius of drawn points.
    :param unique: If True no point can be repeated, else selected points can be repeated.
    :param col_out: outer color of point.
    :param col_in: inner color of point.
    :return:
    """
    # functions
    def getcoor_drawstats(self,points,col_out=black,col_in=green,radius=2):
        """

        :param self:
        :param points:
        :param col_out:
        :param col_in:
        :param radius:
        :return:
        """
        vis = self.rimg
        p = imcoors(points)
        self.data2 = np.zeros((vis.shape[0],vis.shape[1],1),dtype=np.uint8)
        drawcooraxes(vis,[p.boxCenter],col_out,col_in,radius)
        drawcooraxes(self.data2,[p.boxCenter],1,1,self.prox)
        drawcooraxes(vis,[p.mean],col_in,col_out,radius)
        drawcooraxes(self.data2,[p.mean],2,2,self.prox)
        p1 = imcoors(self.coors)
        self.mapdata2 = [None,"center at "+str(p1.boxCenter),"mean at "+str(p1.mean)]

    def getcoor_updatecoors(self):
        """

        :param self:
        :return:
        """
        self.coorlen = len(self.coors)
        self.updaterenderer()
        if self.coors != []:
            self.rcoors = self.coors[:]
            newc = self.coors[:]
            for j,i in enumerate(self.coors):
                newc[j] = self.real2render(i[0],i[1])
                self.rcoors[j] = limitaxispoints(newc[j],10000,-10000)
            if self.showstats:
                getcoor_drawstats(self,newc,radius=self.radius)
            self.rimg = self.updatefunc(self.rimg,newc,self.col_out,self.col_in,self.radius)
        else:
            self.data2[:] = 0
            self.coordinateText = [["xy({self.x},{self.y})"]]

    def getcoor_mouse(self):
        """

        :param self:
        :return:
        """
        # control system
        controlled = self.builtincontrol()
        drawed = False

        # get nearest coordinate to pointer
        isnear = False
        if self.coors != [] and self.rx is not None and self.ry is not None:
            # vals = anorm(np.int32(self.coors) - (self.x, self.y))  # relative to real coordinates
            vals = anorm(np.int32(self.rcoors) - (self.rx, self.ry))  # relative to rendered coordinates
            near_point = np.logical_and(vals < self.prox, vals == np.min(vals))
            if np.any(near_point): # if near point
                idx = np.where(near_point)[0]  # get index
                isnear = True
                val = self.coors[idx[0]]
                count = self.coors.count(val)
                self.coordinateText = [["point "+str(idx[0])+" at "+str(val)+"x"+str(count)]]
            else:
                self.coordinateText = [["xy({self.x},{self.y})"]]

        # coordinate system
        if not controlled and bool(self.flags):
            if self.event== cv2.EVENT_RBUTTONDBLCLK:  # if middle button DELETE ALL COORDINATES
                self.coors = []
                self.img = self.data.copy()
                drawed = True
            elif isnear and self.event== cv2.EVENT_RBUTTONDOWN:  # if right button DELETE NEAREST COORDINATE
                self.coors.pop(idx[0])  # if more than one point delete first
                drawed = True
            elif self.event== cv2.EVENT_LBUTTONDOWN:  # if left button ADD COORDINATE
                val = (self.x,self.y)
                if not self.coors.count(val) or not self.unique:
                    self.coors.append(val)
                drawed = True

        # update renderer
        if (controlled or drawed):
            getcoor_updatecoors(self)

        if self.y is not None and self.x is not None:
            if self.showstats:
                data = self.mapdata2[self.data2[self.ry,self.rx]]
                if not isnear and data is not None:
                    self.coordinateText = [[data]]
            self.builtinplot(self.data[self.y,self.x])

    if type(im) is plotim:
        self = im
    else:
        self = plotim(win,im)
    # assign functions
    self.mousefunc = getcoor_mouse
    self.updatefunc = updatefunc
    self.userupdatefunc = updatefunc
    self.prox = prox  # proximity to keypoint
    # initialize user variables
    self.radius = radius
    self.unique = unique
    self.col_out = col_out
    self.col_in = col_in
    # initialize control variables
    self.interpolation=cv2.INTER_AREA
    self.coors = []
    self.rcoors = []
    self.coorlen = 0
    self.showstats = False
    self.mapdata2 = [None,None,None]
    self.data2 = np.zeros((self.rH,self.rW,1),dtype=np.uint8)
    self.updatecoors = getcoor_updatecoors
    self.drawcooraxes = drawcooraxes
    self.drawcoorperspective = drawcoorperspective
    self.drawcoorpolyline = drawcoorpolyline
    self.drawcoorpoints = drawcoorpoints
    self.controlText[0].extend([" No. coordinates: {self.coorlen}. "])
    self.cmdeval.update({"points":"self.updatefunc = self.drawcoorpoints",
                  "polyline":"self.updatefunc = self.drawcoorpolyline",
                  "perspective":"self.updatefunc = self.drawcoorperspective",
                  "axes":"self.updatefunc = self.drawcooraxes",
                  "user":"self.updatefunc = self.userupdatefunc",
                  "end":["self.getcoor_updatecoors(self)","self.mousefunc(self)"]})
    self.cmdlist.extend(["unique","showstats","user","points","polyline","perspective","axes"])
    # show window
    self.show()
    return self.coors # return coordinates


def stitch_multiple(images = None, **opts):
    """

    :param opts:
    feature = 'sift-flann'
    loader = "400,400"
    saveTo = None,
    autoqualify = False,
    showOnlyPassedTest= False,
    clearAll = False,
    clearData = clearAll,
    clearQualification = clearAll
    :return:
    """
    """
    Notes:
    * inlineratio is really useful to determine if a match is adequate for a merging, but
    it is not good to use when stitching more than 2 images because each time the stitching
    grown the ratios decrease. inlineratio
    """
    from RRtoolbox.lib.descriptors import init_feature, ASIFT_iter,filter_matches

    feature_name = opts.get("feature",'sift-flann')
    saveTo = opts.get("saveTo",None)#"/mnt/4E443F99443F82AF/restoration_data/"
    clearAll = opts.get("clearAll",False)
    if saveTo:
        descriptors_dic = memoizedDict(saveTo+"descriptors")
        data = memoizedDict(saveTo+"data") # this is the result of testRates if it exists
        if opts.get("clearData",clearAll):
            descriptors_dic.clear()
    else:
        descriptors_dic,shapes,data = {},{},{}
    #### LOADING
    if images is None:
        print "looking in path {}".format(MANAGER.TESTPATH)
        fns = glob(MANAGER.TESTPATH + "*.jpg")
    elif isinstance(images,basestring):
        print "looking as {}".format(images)
        fns = glob(images)
    else: # iterator containing data
        fns = images
    #fns = fns[:3]
    print "testing {} files...".format(len(fns))
    #### SCALING
    loader = opts.get("loader",None)
    if isinstance(loader, basestring):
        loader = loadFunc(0,dsize=eval(loader))
    if loader is None:
        rzyf,rzxf = 800,800 # dimensions to scale foregrounds
        loader = loadFunc(0,dsize=(rzxf, rzyf))
    #ims = pathLoader(fns,loader) # load just when needed

    from copy import deepcopy
    with TimeCode("finding descriptors...",endmsg="Overall time is "):
        descriptors_list = []
        """ # this is used if no memoizeDict is used
        for i,(kps,desc) in enumerate(ASIFT_iter(ims,feature_name)):
            descriptors_list.append((len(kps),i,kps,desc))
            if FLAG_DEBUG: print "computing descriptor {}/{}...".format(i,len(ims))"""
        for i, path in enumerate(fns):
            kps,desc = getDicDescriptor(path,loader,feature_name,descriptors_dic)
            for kp in kps:
                kp["modified"] = [] # statistical data
                kp["pt_original"] = kp["pt"]
            descriptors_list.append((len(kps),i,path,kps,desc))
            if FLAG_DEBUG: print "descriptor {}/{}...".format(i+1,len(fns))

    #### MATCHING
    matcher = init_feature(feature_name)[1] # it must get matcher object of cv2 here to prevent conflict with memoizers
    # BFMatcher.knnMatch() returns k best matches where k is specified by the user
    descriptors_list.sort(reverse=True) # descendant: from bigger to least
    _,_,path,kps_base,desc_base = descriptors_list[0] # select first with most descriptors
    used = [path] # select first image path
    failed = [] # registry for failed images
    merged = loader(path) # load first image

    with TimeCode("matching...",endmsg="Overall time is "):
        # TODO make a merger to take a first image and then stitch more to it
        while True:
            kps_remain,desc_remain = [],[] # initialize keypoint and descriptor list of candidates
            for _,_,path,kps,desc in descriptors_list:
                if path not in used: # append only those which are not in the base image
                    kps_remain.extend(kps)
                    desc_remain.extend(desc)

            if not kps_remain: # if there is not image remaining to stitch break
                print "all images used"
                break

            desc_remain = np.array(desc_remain) # convert descriptors to array
            # select only those with good hamming distance
            raw_matches = matcher.knnMatch(desc_remain, trainDescriptors = desc_base, k = 2) #2
            # If k=2, it will draw two match-lines for each keypoint.
            # So we have to pass a status if we want to selectively draw it.
            #p1, p2, kp_pairs = filter_matches(kps_remain, kps_base, raw_matches) #ratio test of 0.75
            # descriptors_dic[kp_pairs[0][0]["name"]][0]
            ratio = 0.75 # filter ratio
            classified = {}
            for m in raw_matches:
                if len(m) == 2 and m[0].distance < m[1].distance * ratio: # by Hamming distance
                    m = m[0]
                    kp1 = kps_remain[m.queryIdx]  # keypoint with Index of the descriptor in query descriptors
                    kp2 = kps_base[m.trainIdx]  # keypoint with Index of the descriptor in train descriptors

                    key = kp1["path"]
                    assert key not in used
                    assert kp2["path"] in used

                    if key in classified:
                        classified[key].append((kp1,kp2))
                    else:
                        classified[key] = [(kp1,kp2)]

            ordered = sorted([(len(v),k) for k,v in classified.items()],reverse=True) # order with best matches

            for v,k in ordered:
                mkp1,mkp2 = zip(*classified[k]) # probably good matches
                p1 = np.float32([kp["pt"] for kp in mkp1])
                p2 = np.float32([kp["pt"] for kp in mkp2])
                H, status = cv2.findHomography(p1, p2, cv2.RANSAC, 5.0)
                if False and H is not None: # test recursivity
                    H, status = cv2.findHomography(np.float32([p for p,s in zip(p1,status) if s]),
                                                   np.float32([p for p,s in zip(p2,status) if s]), cv2.RANSAC, 10.0)
                # FIXME it seems tha when the keypoints correspond to a slanted image the homography cannot
                # minimize the error resulting in erratic transformation functions resulting in it been discarded
                # H should work for the merged image, status specifies the inlier and outlier points
                if H is not None: #first test
                    scaled_fore = loader(k) # load fore image
                    h,w = scaled_fore.shape[:2] #mkp1[0]["shape"][:2]
                    projection = getTransformedCorners((h,w),H) # get corners of dore projection over back
                    c = imcoors(projection) # class to calculate statistical data
                    lines, inlines = len(status), np.sum(status)
                    inlineratio = inlineRatio(inlines,lines) # ratio to determine how good fore is in back
                    text = "inlines/lines: {}/{}={} and rect {}".format(
                        inlines, lines, inlineratio, c.rotatedRectangularity)
                    merged2 = cv2.drawKeypoints(cv2.cvtColor(merged,cv2.COLOR_GRAY2BGR),
                                          [dict2keyPoint(i) for i in kps_base],
                                          flags=4, color=(255,0,0))
                    matchExplorer("match "+text, scaled_fore, merged2, classified[k], status, H)
                    if inlineratio>0.2 and c.rotatedRectangularity>0.5: # second test
                        while True: # clean fail registry
                            try:
                                failed.remove(k)
                            except:
                                break
                        merged, H_back, H_fore = superpose(merged, scaled_fore, H) # create new base
                        #fastplt(merged,"gray")
                        #plotim("last added with "+text,merged).show()
                        projection = getTransformedCorners((h,w),H_fore)
                        newkps, newdesc = [], []
                        '''
                        # newkps, newdesc = [], []
                        # THIS WORKS BUT DOES NOT UPDATE OTHER DATA THAT IS USED FOR VISUALIZATION
                        # MAKING THE NOTION THAT THE ALGORITHM IS NOT WORKING
                        for kp,dsc in zip(kps_base,desc_base):
                            pt = tuple(transformPoint(kp["pt"],H_back))
                            kp["pt"] = pt
                            kp["modified"].append(("H_back",H_back))
                            if cv2.pointPolygonTest(projection, pt, False) == -1: #include only those outside fore
                                newkps.append(kp)
                                newdesc.append(dsc)
                        _,_,_,kps,desc = filter(lambda x: x[2] == k, descriptors_list)[0] # get all the keypoints in that photo
                        # not all fore points are in back
                        for kp in kps: # update keypoint positions
                            kp["pt"] = tuple(transformPoint(kp["pt"],H_fore))
                            kp["modified"].append(("H_fore",H_fore))
                        newkps.extend(kps)
                        newdesc.extend(desc)
                        #kps_base = newkps
                        #desc_base = np.array(newdesc)
                        '''
                        for _,_,path,kps,desc in descriptors_list:
                            if path in used: # append only those which are not in the base image
                                for kp,dsc in zip(kps,desc): # kps,desc
                                    pt = tuple(transformPoint(kp["pt"],H_back))
                                    kp["pt"] = pt
                                    kp["modified"].append(("H_back",H_back))
                                    if cv2.pointPolygonTest(projection, pt, False) == -1: #include only those outside fore
                                        newkps.append(kp)
                                        newdesc.append(dsc)
                            elif path == k:
                                for kp,dsc in zip(kps,desc): # kps,desc
                                    kp["pt"] = tuple(transformPoint(kp["pt"],H_fore))
                                    kp["modified"].append(("H_fore",H_fore))
                                    newkps.append(kp)
                                    newdesc.append(dsc)

                        kps_base = newkps
                        desc_base = np.array(newdesc)
                        used.append(k)
                        assert len(kps_base),len(desc_base)
                        mkp1 = deepcopy(mkp1) # copy keypoints of fore
                        for kp in mkp1:
                            kp["pt"] = kp["pt_original"] # restore its original keypoints for visualization
                        # visualize the match if data is in merged
                        #vis = matchExplorer("match in merged", scaled_fore, merged, zip(mkp1,mkp2), status, H_fore)
                        """
                        # draw keypoints in image
                        plotim("keypoints",
                               cv2.drawKeypoints(cv2.cvtColor(merged,cv2.COLOR_GRAY2BGR),
                                          [dict2keyPoint(i) for i in kps_base],
                                          flags=4, color=(0,0,255))).show()"""
                    else:
                        failed.append(k)
                else:
                    failed.append(k)

            if set(classified.keys()) == set(failed):
                print "Ended, these images do not fit: "
                for i in classified.keys():
                    print i
                break

    base,path,name,ext = getData(used[0])
    name = "merged_"+name
    cv2.imwrite("".join((base,"/home/davtoh/Desktop/",name,ext)),merged)
